{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "> **Github repository**: [02467_Assignment1](https://github.com/JulWin24/02467_Assignment1)\n",
    ">\n",
    "> **Group members**:\n",
    "> - Rune Harlyk (s234814)\n",
    "> - Joseph Nguyen (s234826)\n",
    "> - Julius Winkel (s234862)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from joblib import Parallel, delayed \n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "from ast import literal_eval \n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import netwulf as nw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "from typing import Optional, List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_data(data_file):\n",
    "    if os.path.exists(data_file):\n",
    "        return pd.read_csv(data_file).to_dict(orient=\"records\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ic2s2-2023.org/program\"\n",
    "\n",
    "req = requests.get(url)\n",
    "soup = BeautifulSoup(req.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 1475 plenary names\n",
      "Found: 10 keynotes names\n",
      "Found: 49 chair names\n",
      "Found: 1491 names in total\n"
     ]
    }
   ],
   "source": [
    "names = set()\n",
    "\n",
    "def get_plenary_names(names, soup): \n",
    "    new_names = {name.strip() for nav_list in soup.find_all(\"ul\", class_=\"nav_list\") \n",
    "        for i in nav_list.find_all(\"i\") \n",
    "        for name in i.get_text(strip=True).split(\",\")}\n",
    "    print(f\"Found: {len(new_names)} plenary names\")\n",
    "    names.update(new_names)\n",
    "\n",
    "def get_keynotes_names(names, soup):\n",
    "    new_names = {a.get_text(strip=True).replace(\"Keynote - \", \"\") \n",
    "        for a in soup.find_all(\"a\", href=lambda x: x and x.startswith(\"/keynotes#\"))}\n",
    "    print(f\"Found: {len(new_names)} keynotes names\")\n",
    "    names.update(new_names)\n",
    "    \n",
    "def get_chair_names(names, soup):\n",
    "    new_names = {i.get_text(strip=True).replace(\"Chair: \", \"\") \n",
    "          for i in soup.find_all(\"i\") if i.get_text(strip=True).startswith(\"Chair:\")}\n",
    "    print(f\"Found: {len(new_names)} chair names\")\n",
    "    names.update(new_names)\n",
    "\n",
    "get_plenary_names(names, soup)\n",
    "get_keynotes_names(names, soup)\n",
    "get_chair_names(names, soup)\n",
    "\n",
    "print(f\"Found: {len(names)} names in total\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: 1486 names\n",
      "After fuzzing: 1460 names\n"
     ]
    }
   ],
   "source": [
    "def clean_name(name):\n",
    "    name = unidecode(name)\n",
    "    return name\n",
    "\n",
    "def clean_names(names):\n",
    "    names = {clean_name(name) for name in names}\n",
    "    return names\n",
    "\n",
    "def fuzz_names(names, threshold=90):\n",
    "    names_list = sorted(names)\n",
    "    name_groups = defaultdict(list)\n",
    "\n",
    "    for name in names_list:\n",
    "        first_letter = name[0] if name else \"\"\n",
    "        name_groups[first_letter].append(name)\n",
    "\n",
    "    merge_map = {}\n",
    "    for letter, group in name_groups.items():\n",
    "        for i, name in enumerate(group):\n",
    "            for j in range(i + 1, len(group)):\n",
    "                match_name = group[j]\n",
    "                score = fuzz.ratio(name, match_name)\n",
    "                if score >= threshold:\n",
    "                    merge_map[match_name] = name\n",
    "\n",
    "    merged_names = set()\n",
    "    for name in names_list:\n",
    "        standardized_name = merge_map.get(name, name)\n",
    "        merged_names.add(standardized_name)\n",
    "\n",
    "    return merged_names\n",
    "\n",
    "names = clean_names(names)\n",
    "print(f\"After cleaning: {len(names)} names\")\n",
    "\n",
    "names = fuzz_names(names)\n",
    "print(f\"After fuzzing: {len(names)} names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('author_names_2023.txt', 'w', encoding=\"utf8\") as f:\n",
    "    for name in sorted(names):\n",
    "        f.write(f\"{name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Ready Made vs Custom Made Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book __(answer in max 150 words)__.\n",
    "\n",
    "Damon Centola wanted to test a scenario and obtained custom-made data through the internet. As it is costume made it will be able to tell something about the hypothesis of study. The data will avoid some of the faults of big data, like being 'dirty', 'incomplete' or 'inaccessible'. At the same time the data could also be smaller and more costly. The whole scenario could also be somewhat artificial and might not be applicable in the real world. \n",
    "\n",
    "Sinan Aral and Christos Nicolaides study used ready-made data of 1.1 million users from a fitness app. While the data is nonreactive, there might still be some underlying confounding factor.\n",
    "\n",
    "> 2. How do you think these differences can influence the interpretation of the results in each study? __(answer in max 150 words)__\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSPIRATION:** \n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "\\textbf{Factor} & \\textbf{Centola (Custom-Made)} & \\textbf{Nicolaides (Ready-Made)} \\\\\n",
    "\\hline\n",
    "\\text{Control} & \\text{High – controlled variables} & \\text{Low – cannot manipulate variables} \\\\\n",
    "\\hline\n",
    "\\text{Causality} & \\text{Strong – designed experiment} & \\text{Weak – correlation, not causation} \\\\\n",
    "\\hline\n",
    "\\text{Realism} & \\text{Lower – artificial setting} & \\text{Higher – real-world behaviors} \\\\\n",
    "\\hline\n",
    "\\text{Scale} & \\text{Small – limited participants} & \\text{Large – millions of users} \\\\\n",
    "\\hline\n",
    "\\text{Cost \\& Time} & \\text{High – expensive and time-consuming} & \\text{Low – uses existing data} \\\\\n",
    "\\hline\n",
    "\\text{Data Completeness} & \\text{High – collects exactly what is needed} & \\text{Low – missing key details} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading researches 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded names: 1206\n",
      "After fuzzing: 1202 names\n"
     ]
    }
   ],
   "source": [
    "names_file = \"author_names_2024.txt\"\n",
    "data_file = \"author_data.csv\"\n",
    "\n",
    "with open(names_file, 'r', encoding=\"utf8\") as f:\n",
    "    names = f.read().splitlines()\n",
    "\n",
    "print(f\"Loaded names: {len(names)}\")\n",
    "names = clean_names(names)\n",
    "\n",
    "names = fuzz_names(names)\n",
    "print(f\"After fuzzing: {len(names)} names\")\n",
    "\n",
    "# TODO\n",
    "# 1 - Remove (Santa Fe Institute) from names\n",
    "# 2 - Remove Pensylvania State University from names\n",
    "\n",
    "names = sorted(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining working constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLS\n",
    "WORKS_URL = \"https://api.openalex.org/works\"\n",
    "AUTHORS_URL = \"https://api.openalex.org/authors\"\n",
    "CONCEPTS_URL = \"https://api.openalex.org/concepts\"\n",
    "\n",
    "# REQUESTS PARAMETERS\n",
    "BATCH_SIZE = 25\n",
    "MAX_REQUESTS_PER_SECOND = 10\n",
    "NUM_CORES = 10\n",
    "REQUEST_TIMEOUT = 60\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "# FILTERS\n",
    "social_science_fields = ['Political science', 'Economics', 'Psychology', 'Sociology']\n",
    "quantitative_fields = ['Mathematics', 'Physics', 'Computer science']\n",
    "min_cited_by = 10\n",
    "max_authors = 10\n",
    "\n",
    "# SELECTED FIELDS\n",
    "WORKS_ATTRIBUTES = [\"id\", \"title\", \"publication_year\", \"abstract_inverted_index\", \"authorships\", \"cited_by_count\", \"concepts\"]\n",
    "AUTHOR_ATTRIBUTES = [\"id\", \"display_name\", \"works_count\", \"summary_stats\", \"affiliations\", \"works_api_url\"]\n",
    "\n",
    "# MAPPING\n",
    "id_slice = len(\"https://openalex.org/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to make requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(url: str, mapper = None) -> Optional[Dict]:\n",
    "    retries = 0\n",
    "    while retries <= MAX_RETRIES:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=REQUEST_TIMEOUT)\n",
    "            \n",
    "            # Handle rate limiting and server errors\n",
    "            if response.status_code == 429 or response.status_code >= 500:\n",
    "                wait_time = 0.5 * (2 ** retries)  # Exponential backoff\n",
    "                logger.warning(f\"Request throttled (status {response.status_code}), waiting {wait_time:.2f}s\")\n",
    "                sleep(wait_time)\n",
    "                retries += 1\n",
    "                continue\n",
    "            \n",
    "            if not response.ok:\n",
    "                logger.error(f\"Request failed with status {response.status_code}, {response.text}\")\n",
    "                return None\n",
    "            \n",
    "            if mapper:\n",
    "                return mapper(response.json())\n",
    "            return response.json()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Request error: {e}\")\n",
    "            return None\n",
    "        \n",
    "        time.sleep(1 / MAX_REQUESTS_PER_SECOND) # Apply rate limiting\n",
    "    \n",
    "    logger.error(f\"Max retries exceeded for URL: {url}\")\n",
    "    return None\n",
    "\n",
    "def make_paginated_requests(url: str, mapper = None) -> List[Dict]:\n",
    "    \"\"\"Get all pages of results from paginated API.\"\"\"\n",
    "    all_results = []\n",
    "    cursor = \"*\"\n",
    "    \n",
    "    while cursor:\n",
    "        page_url = f\"{url}&cursor={cursor}\" if \"?\" in url else f\"{url}?cursor={cursor}\"\n",
    "        \n",
    "        response_data = make_request(page_url, mapper)\n",
    "        if not response_data:\n",
    "            break\n",
    "        \n",
    "        results = response_data.get(\"results\", [])\n",
    "        if mapper:\n",
    "            mapped_results = []\n",
    "            for item in results:\n",
    "                try:\n",
    "                    mapped_item = mapper(item)\n",
    "                    if mapped_item is not None:\n",
    "                        mapped_results.append(mapped_item)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in mapper function: {e}\")\n",
    "            all_results.extend(mapped_results)\n",
    "        else:\n",
    "            all_results.extend(results)\n",
    "\n",
    "        cursor = response_data.get(\"meta\", {}).get(\"next_cursor\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching of IC2S2 2024 author data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_author_result(results: Dict) -> Dict:\n",
    "    return {\n",
    "        \"id\": results.get(\"id\")[id_slice:],\n",
    "        \"display_name\": results.get(\"display_name\"),\n",
    "        \"works_count\": results.get(\"works_count\"),\n",
    "        \"h_index\": results.get(\"summary_stats\")[\"h_index\"],\n",
    "        \"country_code\": results.get(\"affiliations\")[0][\"institution\"][\"country_code\"],\n",
    "        \"works_api_url\": results.get(\"works_api_url\")\n",
    "    }\n",
    "\n",
    "def map_first_author(json: Dict) -> Dict:\n",
    "    res = json.get(\"results\")[0]\n",
    "    return map_author_result(res)\n",
    "\n",
    "def get_author_data(name):\n",
    "    try:\n",
    "        url = f\"https://api.openalex.org/authors?filter=display_name.search:{name}\"\n",
    "        author = make_request(url, map_first_author)\n",
    "        return author if author else name\n",
    "    except Exception as ex:\n",
    "        print(f\"Error: {ex}\")\n",
    "        return name\n",
    "    \n",
    "# get_author_data(\"Ralph Hertwig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already have 0, missing 1202\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ceeeb993e92492aa75e161eebf6a367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching authors in parallel:   0%|          | 0/1202 [00:00<?, ?authors/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got data for: 1025, missing 177\n"
     ]
    }
   ],
   "source": [
    "existing_data = load_existing_data(data_file)\n",
    "existing_names = {entry['display_name'] for entry in existing_data if 'display_name' in entry}\n",
    "names_to_process = list(set(names) - existing_names)\n",
    "\n",
    "print(f\"Already have {len(existing_names)}, missing {len(names_to_process)}, total {len(names)}\")\n",
    "\n",
    "author_data = existing_data\n",
    "bad_names = []\n",
    "\n",
    "results = Parallel(n_jobs=NUM_CORES)(\n",
    "    delayed(get_author_data)(name) for name in tqdm(names_to_process, desc=\"Fetching authors in parallel\", unit=\"authors\")\n",
    ")\n",
    "\n",
    "author_data = [res for res in results if isinstance(res, dict)]\n",
    "\n",
    "bad_names = [res for res in results if not isinstance(res, dict)]\n",
    "\n",
    "author_df = pd.DataFrame(existing_data + author_data)\n",
    "\n",
    "author_df = author_df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "author_df.to_csv(data_file, index=False)\n",
    "\n",
    "print(f\"Got data for: {len(author_data)}, missing {len(bad_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data again and filter between 5-5000 works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025\n",
      "917\n"
     ]
    }
   ],
   "source": [
    "author_df = pd.read_csv('author_data.csv')\n",
    "author_df = author_df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "print(len(author_df))\n",
    "author_df = author_df[(author_df[\"works_count\"] >= 5) & (author_df[\"works_count\"] <= 5000)]\n",
    "print(len(author_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_url(level:int = 0) -> str:\n",
    "    return f\"{CONCEPTS_URL}?filter=level:{level}&per-page=200\"\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def fetch_concept_ids(level = 0) -> str:\n",
    "    concepts_url = get_concepts_url(level)\n",
    "    response_concepts = requests.get(concepts_url)\n",
    "\n",
    "    if response_concepts.ok:\n",
    "        concepts = response_concepts.json()['results']\n",
    "        \n",
    "        social_science_ids = [i['id'][id_slice:] for i in concepts if i['display_name'] in social_science_fields]\n",
    "        quantitative_ids = [i['id'][id_slice:] for i in concepts if i['display_name'] in quantitative_fields]\n",
    "\n",
    "    return social_science_ids, quantitative_ids\n",
    "\n",
    "social_science_ids, quantitative_ids = fetch_concept_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_concept_filter(*groups: List[List[str]]) -> str:\n",
    "    return \",\".join((f\"concepts.id:{'|'.join(group)}\" for group in groups))\n",
    "\n",
    "def create_cited_by_filter(min_cited_by):\n",
    "    return f\"cited_by_count:>{min_cited_by}\"\n",
    "\n",
    "def create_authors_filter(ids: List[str]) -> str:\n",
    "    return f\"authorships.author.id:{'|'.join(ids)}\"\n",
    "\n",
    "def create_author_id_filter(ids: List[str]) -> str:\n",
    "    return f\"id:{'|'.join(ids)}\"\n",
    "\n",
    "def create_author_count_filter(max_authors):\n",
    "    return f\"authors_count:<{max_authors}\"\n",
    "\n",
    "def create_query_filter(*filters:List[str]) -> str:\n",
    "    return \",\".join(filters)\n",
    "\n",
    "social_science_ids, quantitative_ids = fetch_concept_ids()\n",
    "concept_filter = create_concept_filter(social_science_ids, quantitative_ids)\n",
    "cited_by_filter = create_cited_by_filter(min_cited_by)\n",
    "author_count_filter = create_author_count_filter(max_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(base_url: str, filter_str:str, select_data: List[str], per_page:int = 200) -> str:\n",
    "    return (\n",
    "        f\"{base_url}?filter={filter_str}\"  # Filter data\n",
    "        f\"&select={','.join(select_data)}\"  # Select data\n",
    "        f\"&per_page={per_page}\"             # Fetch max results per request\n",
    "    )\n",
    "\n",
    "def get_works_url(filter_str:str, select_data: List[str], per_page:int = 200) -> str:\n",
    "    return get_url(WORKS_URL, filter_str, select_data, per_page)\n",
    "\n",
    "def get_author_url(filter_str:str, select_data: List[str], per_page:int = 200) -> str:\n",
    "    return get_url(AUTHORS_URL, filter_str, select_data, per_page)\n",
    "\n",
    "# author_count_filter = create_authors_filter([\"A5068556395\"])\n",
    "# query_filter = create_query_filter(concept_filter, cited_by_filter, author_count_filter) \n",
    "\n",
    "# test_works_url = get_works_url(query_filter, WORKS_ATTRIBUTES)\n",
    "# test_works_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_work(item) -> tuple[dict, dict]:\n",
    "    return {\n",
    "        \"id\": item[\"id\"],\n",
    "        \"publication_year\": item.get(\"publication_year\"),\n",
    "        \"cited_by_count\": item.get(\"cited_by_count\", 0),\n",
    "        \"author_ids\": [auth[\"author\"][\"id\"][id_slice:] for auth in item.get(\"authorships\", [])]\n",
    "    }, {\n",
    "        \"id\": item[\"id\"],\n",
    "        \"title\": item.get(\"title\"),\n",
    "        \"abstract_inverted_index\": item.get(\"abstract_inverted_index\")\n",
    "    }\n",
    "\n",
    "def fetch_work_batched(authors):\n",
    "    author_filter = create_authors_filter(authors)\n",
    "    query_filter = create_query_filter(concept_filter, cited_by_filter, author_count_filter, author_filter) \n",
    "    url = get_works_url(query_filter, WORKS_ATTRIBUTES)\n",
    "\n",
    "    all_papers = []\n",
    "    all_abstracts = []\n",
    "\n",
    "    def process_work(work):\n",
    "        papers, abstracts = map_work(work)\n",
    "        all_papers.append(papers)\n",
    "        all_abstracts.append(abstracts)\n",
    "        return None\n",
    "\n",
    "    make_paginated_requests(url, mapper=process_work)\n",
    "\n",
    "    return all_papers, all_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching works for 917 authors in 37 batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9b48d09e3e49af9e87459cc03033f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching works in parallel:   0%|          | 0/37 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching 37 results\n"
     ]
    }
   ],
   "source": [
    "author_ids = author_df[\"id\"].tolist()\n",
    "author_batches = [author_ids[i: i + BATCH_SIZE] for i in range(0, len(author_ids), BATCH_SIZE)]\n",
    "\n",
    "print(f\"Fetching works for {len(author_ids)} authors in {len(author_batches)} batches\")\n",
    "\n",
    "results = Parallel(n_jobs=NUM_CORES)(\n",
    "    delayed(fetch_work_batched)(batch) for batch in tqdm(author_batches, desc=\"Fetching works in parallel\", unit=\"batch\")\n",
    ")\n",
    "\n",
    "print(f\"Finished fetching {len(results)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 11351 papers and 11351 abstracts\n"
     ]
    }
   ],
   "source": [
    "all_papers = [paper for batch_papers, _ in results for paper in batch_papers]\n",
    "all_abstracts = [abstract for _, batch_abstracts in results for abstract in batch_abstracts]\n",
    "\n",
    "print(f\"Got {len(all_papers)} papers and {len(all_abstracts)} abstracts\")\n",
    "\n",
    "papers_df = pd.DataFrame(all_papers)\n",
    "abstracts_df = pd.DataFrame(all_abstracts)\n",
    "\n",
    "papers_df = papers_df.drop_duplicates(subset='id', keep='first')\n",
    "abstracts_df = abstracts_df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "papers_df.to_csv(\"ic2s2_papers.csv\", index=False)\n",
    "abstracts_df.to_csv(\"ic2s2_abstract.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all co authors from papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15324"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df = pd.read_csv(\"ic2s2_papers.csv\", converters={'author_ids': literal_eval})\n",
    "coauthor_ids = papers_df.explode('author_ids')[\"author_ids\"].unique().tolist()\n",
    "len(set(coauthor_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_author_result_filter(result: Dict) -> Dict:\n",
    "    num_works = result.get('works_count', 0)\n",
    "    if num_works < 5 or num_works > 5000:\n",
    "        return None\n",
    "    return map_author_result(result)\n",
    "\n",
    "def fetch_author_batched(authors):\n",
    "    author_filter = create_author_id_filter(authors)\n",
    "    print(author_filter)\n",
    "    query_filter = create_query_filter(author_filter) \n",
    "    url = get_author_url(query_filter, AUTHOR_ATTRIBUTES)\n",
    "\n",
    "    return make_paginated_requests(url, mapper=map_author_result_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching author data for 15324 authors in 613 batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e08a804ace4d9ba087e12176b172be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching author in parallel:   0%|          | 0/613 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching 613 results\n"
     ]
    }
   ],
   "source": [
    "author_batches = [coauthor_ids[i: i + BATCH_SIZE] for i in range(0, len(coauthor_ids), BATCH_SIZE)]\n",
    "\n",
    "print(f\"Fetching coauthor data for {len(coauthor_ids)} authors in {len(author_batches)} batches\")\n",
    "\n",
    "results = Parallel(n_jobs=NUM_CORES)(\n",
    "    delayed(fetch_author_batched)(batch) for batch in tqdm(author_batches, desc=\"Fetching author in parallel\", unit=\"batch\")\n",
    ")\n",
    "\n",
    "print(f\"Finished fetching {len(results)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got data for 14061 coauthors\n"
     ]
    }
   ],
   "source": [
    "coauthors = [author for batch in results for author in batch]\n",
    "\n",
    "coauthors_df = pd.DataFrame(coauthors)\n",
    "coauthors_df = coauthors_df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "coauthors_df.to_csv(\"ic2s2_coauthors.csv\", index=False)\n",
    "\n",
    "print(f\"Got data for {len(coauthors_df)} coauthors\")\n",
    "# coauthors_df[\"works_count\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Network of Computational Social Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting final dataset with authors and coauthors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14293"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_authors_df = pd.concat([author_df, coauthors_df], ignore_index=True)\n",
    "all_authors_df = all_authors_df.drop_duplicates(subset='id', keep='first')\n",
    "all_authors_df.to_csv(\"ic2s2_all_authors.csv\", index=False)\n",
    "all_author_ids = all_authors_df.explode('id')[\"id\"].unique().tolist()\n",
    "len(all_author_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 572 batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f967a18760405f9171e1c8c54153ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching works in parallel:   0%|          | 0/572 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_author_batches_ids = [all_author_ids[i: i + BATCH_SIZE] for i in range(0, len(all_author_ids), BATCH_SIZE)]\n",
    "\n",
    "print(f\"Fetching {len(all_author_ids)} authors in {len(all_author_batches_ids)} batches\")\n",
    "\n",
    "results = Parallel(n_jobs=NUM_CORES)(\n",
    "    delayed(fetch_work_batched)(batch) for batch in tqdm(all_author_batches_ids, desc=\"Fetching works in parallel\", unit=\"batch\")\n",
    ")\n",
    "\n",
    "## TODO - FILTER THAT ONLY AUTHORS IN I2CS2 ARE CONSIDERED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers = [paper for batch_papers, _ in results for paper in batch_papers]\n",
    "all_abstracts = [abstract for _, batch_abstracts in results for abstract in batch_abstracts]\n",
    "\n",
    "# Convert to DataFrame\n",
    "all_papers_df = pd.DataFrame(all_papers)\n",
    "all_abstracts_df = pd.DataFrame(all_abstracts)\n",
    "\n",
    "# Drop\n",
    "all_papers_df = all_papers_df.drop_duplicates(subset='id', keep='first')\n",
    "all_abstracts_df = all_abstracts_df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "all_papers_df.to_csv(\"all_papers.csv\", index=False)\n",
    "all_abstracts_df.to_csv(\"all_abstracts.csv\", index=False)\n",
    "len(all_papers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to only use 2 degrees of separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37912"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out papers with authors not in the author list\n",
    "all_unique_author_ids = set(all_author_ids)\n",
    "all_papers_df[\"author_ids\"] = all_papers_df[\"author_ids\"].apply(lambda x: [i for i in x if i in all_unique_author_ids])\n",
    "\n",
    "all_papers_df = all_papers_df[all_papers_df[\"author_ids\"].apply(len) >= 2]\n",
    "\n",
    "all_papers_df.to_csv(\"ic2s2_coauthors_papers.csv\", index=False)\n",
    "\n",
    "len(all_papers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Network Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting author pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63718"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = Counter()\n",
    "\n",
    "for author_list in all_papers_df[\"author_ids\"]:\n",
    "    for pair in combinations(author_list, 2):\n",
    "        edges[pair] += 1\n",
    "\n",
    "edgelist = [(a, b, count) for (a, b), count in edges.items()]\n",
    "len(edgelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graph(graph_file, G):\n",
    "    data = nx.readwrite.json_graph.node_link_data(G)\n",
    "    with open(graph_file, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def load_graph(graph_file):\n",
    "    with open(graph_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return nx.readwrite.json_graph.node_link_graph(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = all_papers_df.explode(\"author_ids\")\n",
    "\n",
    "author_stats = df_exploded.groupby(\"author_ids\").agg(\n",
    "    first_publication_year=(\"publication_year\", \"min\"),\n",
    "    cited_by_count=(\"cited_by_count\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "df_merged = all_authors_df.merge(author_stats, left_on=\"id\", right_on=\"author_ids\", how=\"inner\")\n",
    "df_merged.drop(columns=[\"author_ids\"], inplace=True)\n",
    "attr_dict = df_merged[[\"id\", \"display_name\", \"country_code\", \"first_publication_year\", \"cited_by_count\"]].set_index(\"id\").to_dict(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_file = \"ic2s2_coauthors_graph.json\"\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(edgelist)\n",
    "nx.set_node_attributes(G, attr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_graph(graph_file, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preliminary Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 63718 links between 14293 nodes\n",
      "Network density is: 0.0005797675391076178\n",
      "Is fully connected:  False\n",
      "Number of connected components:  89\n",
      "Number of isolated nodes:  0\n"
     ]
    }
   ],
   "source": [
    "# Network Stats\n",
    "num_links = len(edgelist)\n",
    "num_nodes = len(all_unique_author_ids)\n",
    "print(f\"Got {num_links} links between {num_nodes} nodes\")\n",
    "\n",
    "# Density Stats\n",
    "print(f'Network density is: {nx.density(G)}')\n",
    "\n",
    "# Number of connected components\n",
    "num_isolated = len(list(nx.isolates(G)))\n",
    "print(\"Is fully connected: \", nx.is_connected(G))\n",
    "print(\"Number of connected components: \", nx.number_connected_components(G))\n",
    "print(\"Number of isolated nodes: \", num_isolated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg': 8.146313692001138, 'median': 6.0, 'mode': 4, 'min': 1, 'max': 339}\n",
      "{'avg': 12.808995160831198, 'median': 7.0, 'mode': 4, 'min': 1, 'max': 536}\n"
     ]
    }
   ],
   "source": [
    "degrees = [d for _, d in G.degree()]\n",
    "strengths = [s for _, s in G.degree(weight=\"weight\")]\n",
    "\n",
    "degree_stats = {\n",
    "    \"avg\": np.mean(degrees),\n",
    "    \"median\": np.median(degrees),\n",
    "    \"mode\": Counter(degrees).most_common(1)[0][0],\n",
    "    \"min\": np.min(degrees),\n",
    "    \"max\": np.max(degrees)\n",
    "}\n",
    "\n",
    "strength_stats = {\n",
    "    \"avg\": np.mean(strengths),\n",
    "    \"median\": np.median(strengths),\n",
    "    \"mode\": Counter(strengths).most_common(1)[0][0],\n",
    "    \"min\": np.min(strengths),\n",
    "    \"max\": np.max(strengths)\n",
    "}\n",
    "\n",
    "print(degree_stats)\n",
    "print(strength_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A5100322712', 339), ('A5005421447', 302), ('A5077712228', 270), ('A5007176508', 239), ('A5059645286', 235)]\n"
     ]
    }
   ],
   "source": [
    "def top_nodes_by_degree(G, top_n=5):\n",
    "    return sorted(G.degree, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "top_5 = top_nodes_by_degree(G)\n",
    "print(top_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Out of range float values are not JSON compliant",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[308], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m id_to_name \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(all_authors_df\u001b[38;5;241m.\u001b[39mdisplay_name\u001b[38;5;241m.\u001b[39mvalues, index\u001b[38;5;241m=\u001b[39mall_authors_df\u001b[38;5;241m.\u001b[39mid)\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# G_named = nx.relabel_nodes(G, id_to_name)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m network, config \u001b[38;5;241m=\u001b[39m \u001b[43mnw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, config=config)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# fig, ax = nw.draw_netwulf(network, figsize=(10,10))\u001b[39;00m\n\u001b[0;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\netwulf\\interactive.py:278\u001b[0m, in \u001b[0;36mvisualize\u001b[1;34m(network, port, verbose, config, plot_in_cell_below, is_test)\u001b[0m\n\u001b[0;32m    276\u001b[0m         network\u001b[38;5;241m.\u001b[39mupdate(network[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m network[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 278\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable_as_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_json_default\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(network) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m    280\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(network, f, iterable_as_array\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, default\u001b[38;5;241m=\u001b[39m_json_default)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\simplejson\\__init__.py:269\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, encoding, default, use_decimal, namedtuple_as_object, tuple_as_array, bigint_as_string, sort_keys, item_sort_key, for_json, ignore_nan, int_as_string_bitcount, iterable_as_array, **kw)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[0;32m    256\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_decimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_decimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnamedtuple_as_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamedtuple_as_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtuple_as_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtuple_as_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43miterable_as_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miterable_as_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbigint_as_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbigint_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mitem_sort_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem_sort_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfor_json\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfor_json\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mint_as_string_bitcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mint_as_string_bitcount\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m--> 269\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\simplejson\\encoder.py:379\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    370\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[0;32m    371\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_sort_key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfor_json,\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterable_as_array, Decimal\u001b[38;5;241m=\u001b[39mdecimal\u001b[38;5;241m.\u001b[39mDecimal)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m     key_memo\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[1;31mValueError\u001b[0m: Out of range float values are not JSON compliant"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"zoom\": 0.6,\n",
    "    \"scale_node_size_by_strength\": True,\n",
    "    \"node_size_variation\": 1,\n",
    "    \"node_size\": 30,\n",
    "    \"node_gravity\": 0.45,\n",
    "}\n",
    "\n",
    "id_to_name = pd.Series(all_authors_df.display_name.values, index=all_authors_df.id).to_dict()\n",
    "\n",
    "# G_named = nx.relabel_nodes(G, id_to_name)\n",
    "\n",
    "network, config = nw.visualize(G)#, config=config)\n",
    "\n",
    "# fig, ax = nw.draw_netwulf(network, figsize=(10,10))\n",
    "plt.show()\n",
    "# plt.savefig(\"myfigure.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
