{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "> **Github repository**: [02467_Assignment1](https://github.com/JulWin24/02467_Assignment1)\n",
    ">\n",
    "> **Group members**:\n",
    "> - Rune Harlyk (s234814)\n",
    "> - Joseph Nguyen (s234826)\n",
    "> - Julius Winkel (s234862)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from joblib import Parallel, delayed \n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "from ast import literal_eval \n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import netwulf as nw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_data(data_file):\n",
    "    if os.path.exists(data_file):\n",
    "        return pd.read_csv(data_file).to_dict(orient=\"records\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ic2s2-2023.org/program\"\n",
    "\n",
    "req = requests.get(url)\n",
    "soup = BeautifulSoup(req.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 1475 plenary names\n",
      "Found: 10 keynotes names\n",
      "Found: 49 chair names\n",
      "Found: 1491 names in total\n"
     ]
    }
   ],
   "source": [
    "names = set()\n",
    "\n",
    "def get_plenary_names(names, soup): \n",
    "    new_names = {name.strip() for nav_list in soup.find_all(\"ul\", class_=\"nav_list\") \n",
    "        for i in nav_list.find_all(\"i\") \n",
    "        for name in i.get_text(strip=True).split(\",\")}\n",
    "    print(f\"Found: {len(new_names)} plenary names\")\n",
    "    names.update(new_names)\n",
    "\n",
    "def get_keynotes_names(names, soup):\n",
    "    new_names = {a.get_text(strip=True).replace(\"Keynote - \", \"\") \n",
    "        for a in soup.find_all(\"a\", href=lambda x: x and x.startswith(\"/keynotes#\"))}\n",
    "    print(f\"Found: {len(new_names)} keynotes names\")\n",
    "    names.update(new_names)\n",
    "    \n",
    "def get_chair_names(names, soup):\n",
    "    new_names = {i.get_text(strip=True).replace(\"Chair: \", \"\") \n",
    "          for i in soup.find_all(\"i\") if i.get_text(strip=True).startswith(\"Chair:\")}\n",
    "    print(f\"Found: {len(new_names)} chair names\")\n",
    "    names.update(new_names)\n",
    "\n",
    "get_plenary_names(names, soup)\n",
    "get_keynotes_names(names, soup)\n",
    "get_chair_names(names, soup)\n",
    "\n",
    "print(f\"Found: {len(names)} names in total\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: 1486 names\n",
      "After fuzzing: 1460 names\n"
     ]
    }
   ],
   "source": [
    "def clean_name(name):\n",
    "    name = unidecode(name)\n",
    "    return name\n",
    "\n",
    "def clean_names(names):\n",
    "    names = {clean_name(name) for name in names}\n",
    "    return names\n",
    "\n",
    "def fuzz_names(names, threshold=90):\n",
    "    names_list = sorted(names)\n",
    "    name_groups = defaultdict(list)\n",
    "\n",
    "    for name in names_list:\n",
    "        first_letter = name[0] if name else \"\"\n",
    "        name_groups[first_letter].append(name)\n",
    "\n",
    "    merge_map = {}\n",
    "    for letter, group in name_groups.items():\n",
    "        for i, name in enumerate(group):\n",
    "            for j in range(i + 1, len(group)):\n",
    "                match_name = group[j]\n",
    "                score = fuzz.ratio(name, match_name)\n",
    "                if score >= threshold:\n",
    "                    merge_map[match_name] = name\n",
    "\n",
    "    merged_names = set()\n",
    "    for name in names_list:\n",
    "        standardized_name = merge_map.get(name, name)\n",
    "        merged_names.add(standardized_name)\n",
    "\n",
    "    return merged_names\n",
    "\n",
    "names = clean_names(names)\n",
    "print(f\"After cleaning: {len(names)} names\")\n",
    "\n",
    "names = fuzz_names(names)\n",
    "print(f\"After fuzzing: {len(names)} names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('author_names_2023.txt', 'w', encoding=\"utf8\") as f:\n",
    "    for name in sorted(names):\n",
    "        f.write(f\"{name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Ready Made vs Custom Made Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book __(answer in max 150 words)__.\n",
    "\n",
    "Damon Centola wanted to test a scenario and obtained custom-made data through the internet. As it is costume made it will be able to tell something about the hypothesis of study. The data will avoid some of the faults of big data, like being 'dirty', 'incomplete' or 'inaccessible'. At the same time the data could also be smaller and more costly. The whole scenario could also be somewhat artificial and might not be applicable in the real world. \n",
    "\n",
    "Sinan Aral and Christos Nicolaides study used ready-made data of 1.1 million users from a fitness app. While the data is nonreactive, there might still be some underlying confounding factor.\n",
    "\n",
    "> 2. How do you think these differences can influence the interpretation of the results in each study? __(answer in max 150 words)__\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSPIRATION:** \n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "\\textbf{Factor} & \\textbf{Centola (Custom-Made)} & \\textbf{Nicolaides (Ready-Made)} \\\\\n",
    "\\hline\n",
    "\\text{Control} & \\text{High – controlled variables} & \\text{Low – cannot manipulate variables} \\\\\n",
    "\\hline\n",
    "\\text{Causality} & \\text{Strong – designed experiment} & \\text{Weak – correlation, not causation} \\\\\n",
    "\\hline\n",
    "\\text{Realism} & \\text{Lower – artificial setting} & \\text{Higher – real-world behaviors} \\\\\n",
    "\\hline\n",
    "\\text{Scale} & \\text{Small – limited participants} & \\text{Large – millions of users} \\\\\n",
    "\\hline\n",
    "\\text{Cost \\& Time} & \\text{High – expensive and time-consuming} & \\text{Low – uses existing data} \\\\\n",
    "\\hline\n",
    "\\text{Data Completeness} & \\text{High – collects exactly what is needed} & \\text{Low – missing key details} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading researches 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded names: 1206\n",
      "After fuzzing: 1202 names\n"
     ]
    }
   ],
   "source": [
    "names_file = \"author_names_2024.txt\"\n",
    "data_file = \"author_data.csv\"\n",
    "\n",
    "with open(names_file, 'r', encoding=\"utf8\") as f:\n",
    "    names = f.read().splitlines()\n",
    "\n",
    "print(f\"Loaded names: {len(names)}\")\n",
    "names = clean_names(names)\n",
    "\n",
    "names = fuzz_names(names)\n",
    "print(f\"After fuzzing: {len(names)} names\")\n",
    "\n",
    "# TODO\n",
    "# 1 - Remove (Santa Fe Institute) from names\n",
    "# 2 - Remove Pensylvania State University from names\n",
    "\n",
    "names = sorted(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining working constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URLS\n",
    "WORKS_URL = \"https://api.openalex.org/works\"\n",
    "AUTHORS_URL = \"https://api.openalex.org/authors\"\n",
    "CONCEPTS_URL = \"https://api.openalex.org/concepts\"\n",
    "\n",
    "# REQUESTS PARAMETERS\n",
    "BATCH_SIZE = 25\n",
    "MAX_REQUESTS_PER_SECOND = 10\n",
    "NUM_CORES = 10\n",
    "REQUEST_TIMEOUT = 60\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "# FILTERS\n",
    "social_science_fields = ['Political science', 'Economics', 'Psychology', 'Sociology']\n",
    "quantitative_fields = ['Mathematics', 'Physics', 'Computer science']\n",
    "min_cited_by = 10\n",
    "max_authors = 10\n",
    "\n",
    "# SELECTED FIELDS\n",
    "WORKS_ATTRIBUTES = [\"id\", \"title\", \"publication_year\", \"abstract_inverted_index\", \"authorships\", \"cited_by_count\", \"concepts\"]\n",
    "AUTHOR_ATTRIBUTES = [\"id\", \"display_name\", \"works_count\", \"h_index\", \"country_code\", \"works_api_url\"]\n",
    "\n",
    "# MAPPING\n",
    "id_slice = len(\"https://openalex.org/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to make requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(url: str, mapper = None) -> Optional[Dict]:\n",
    "    retries = 0\n",
    "    while retries <= MAX_RETRIES:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=REQUEST_TIMEOUT)\n",
    "            \n",
    "            # Handle rate limiting and server errors\n",
    "            if response.status_code == 429 or response.status_code >= 500:\n",
    "                wait_time = 0.5 * (2 ** retries)  # Exponential backoff\n",
    "                logger.warning(f\"Request throttled (status {response.status_code}), waiting {wait_time:.2f}s\")\n",
    "                sleep(wait_time)\n",
    "                retries += 1\n",
    "                continue\n",
    "            \n",
    "            if not response.ok:\n",
    "                logger.error(f\"Request failed with status {response.status_code}, {response.text}\")\n",
    "                return None\n",
    "            \n",
    "            if mapper:\n",
    "                return mapper(response.json())\n",
    "            return response.json()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Request error: {e}\")\n",
    "            return None\n",
    "        \n",
    "        time.sleep(1 / MAX_REQUESTS_PER_SECOND) # Apply rate limiting\n",
    "    \n",
    "    logger.error(f\"Max retries exceeded for URL: {url}\")\n",
    "    return None\n",
    "\n",
    "def make_paginated_requests(url: str, mapper = None) -> List[Dict]:\n",
    "    \"\"\"Get all pages of results from paginated API.\"\"\"\n",
    "    all_results = []\n",
    "    cursor = \"*\"\n",
    "    \n",
    "    while cursor:\n",
    "        page_url = f\"{url}&cursor={cursor}\" if \"?\" in url else f\"{url}?cursor={cursor}\"\n",
    "        \n",
    "        response_data = make_request(page_url)\n",
    "        if not response_data:\n",
    "            break\n",
    "        \n",
    "        results = response_data.get(\"results\", [])\n",
    "        if mapper:\n",
    "            mapped_results = []\n",
    "            for item in results:\n",
    "                try:\n",
    "                    mapped_item = mapper(item)\n",
    "                    if mapped_item is not None:\n",
    "                        mapped_results.append(mapped_item)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in mapper function: {e}\")\n",
    "            all_results.extend(mapped_results)\n",
    "        else:\n",
    "            all_results.extend(results)\n",
    "\n",
    "        cursor = response_data.get(\"meta\", {}).get(\"next_cursor\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching of IC2S2 2024 author data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_author_result(results: Dict) -> Dict:\n",
    "    return {\n",
    "        \"id\": results.get(\"id\")[id_slice:],\n",
    "        \"display_name\": results.get(\"display_name\"),\n",
    "        \"works_count\": results.get(\"works_count\"),\n",
    "        \"h_index\": results.get(\"summary_stats\")[\"h_index\"],\n",
    "        \"country_code\": results.get(\"affiliations\")[0][\"institution\"][\"country_code\"],\n",
    "        \"works_api_url\": results.get(\"works_api_url\")\n",
    "    }\n",
    "\n",
    "def map_first_author(json: Dict) -> Dict:\n",
    "    res = json.get(\"results\")[0]\n",
    "    return map_author_result(res)\n",
    "\n",
    "def get_author_data(name):\n",
    "    try:\n",
    "        url = f\"https://api.openalex.org/authors?filter=display_name.search:{name}\"\n",
    "        author = make_request(url, map_first_author)\n",
    "        return author if author else name\n",
    "    except Exception as ex:\n",
    "        print(f\"Error: {ex}\")\n",
    "        return name\n",
    "    \n",
    "# get_author_data(\"Ralph Hertwig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already have 0, missing 1202\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ceeeb993e92492aa75e161eebf6a367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching authors in parallel:   0%|          | 0/1202 [00:00<?, ?authors/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got data for: 1025, missing 177\n"
     ]
    }
   ],
   "source": [
    "existing_data = load_existing_data(data_file)\n",
    "existing_names = {entry['display_name'] for entry in existing_data if 'display_name' in entry}\n",
    "names_to_process = list(set(names) - existing_names)\n",
    "\n",
    "print(f\"Already have {len(existing_names)}, missing {len(names_to_process)}, total {len(names)}\")\n",
    "\n",
    "author_data = existing_data\n",
    "bad_names = []\n",
    "\n",
    "results = Parallel(n_jobs=NUM_CORES)(\n",
    "    delayed(get_author_data)(name) for name in tqdm(names_to_process, desc=\"Fetching authors in parallel\", unit=\"authors\")\n",
    ")\n",
    "\n",
    "author_data = [res for res in results if isinstance(res, dict)]\n",
    "\n",
    "bad_names = [res for res in results if not isinstance(res, dict)]\n",
    "\n",
    "author_df = pd.DataFrame(existing_data + author_data)\n",
    "\n",
    "author_df = author_df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "author_df.to_csv(data_file, index=False)\n",
    "\n",
    "print(f\"Got data for: {len(author_data)}, missing {len(bad_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data again and filter between 5-5000 works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025\n",
      "917\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('author_data.csv')\n",
    "df = df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "print(len(df))\n",
    "df = df[(df[\"works_count\"] >= 5) & (df[\"works_count\"] <= 5000)]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKS_ATTRIBUTES = [\"id\", \"title\", \"publication_year\", \"abstract_inverted_index\", \"authorships\", \"cited_by_count\", \"concepts\"]\n",
    "AUTHOR_ATTRIBUTES = [\"id\", \"display_name\", \"works_count\", \"h_index\", \"country_code\", \"works_api_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concepts_url(level:int = 0) -> str:\n",
    "    return f\"{CONCEPTS_URL}?filter=level:{level}&per-page=200\"\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def fetch_concept_ids(level = 0) -> str:\n",
    "    concepts_url = get_concepts_url(level)\n",
    "    response_concepts = requests.get(concepts_url)\n",
    "\n",
    "    if response_concepts.ok:\n",
    "        concepts = response_concepts.json()['results']\n",
    "        \n",
    "        social_science_ids = [i['id'][id_slice:] for i in concepts if i['display_name'] in social_science_fields]\n",
    "        quantitative_ids = [i['id'][id_slice:] for i in concepts if i['display_name'] in quantitative_fields]\n",
    "\n",
    "    return social_science_ids, quantitative_ids\n",
    "\n",
    "social_science_ids, quantitative_ids = fetch_concept_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_concept_filter(*groups: List[List[str]]) -> str:\n",
    "    return \",\".join((f\"concepts.id:{'|'.join(group)}\" for group in groups))\n",
    "\n",
    "def create_cited_by_filter(min_cited_by):\n",
    "    return f\"cited_by_count:>{min_cited_by}\"\n",
    "\n",
    "def create_authors_filter(ids: List[str]) -> str:\n",
    "    return f\"authorships.author.id:{'|'.join(ids)}\"\n",
    "\n",
    "def create_author_count_filter(max_authors):\n",
    "    return f\"authors_count:<{max_authors}\"\n",
    "\n",
    "def create_query_filter(*filters:List[str]) -> str:\n",
    "    return \",\".join(filters)\n",
    "\n",
    "social_science_ids, quantitative_ids = fetch_concept_ids()\n",
    "concept_filter = create_concept_filter(social_science_ids, quantitative_ids)\n",
    "cited_by_filter = create_cited_by_filter(min_cited_by)\n",
    "author_count_filter = create_author_count_filter(max_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_works_url(filter_str:str, select_data: List[str], per_page:int = 200) -> str:\n",
    "    return (\n",
    "            f\"{WORKS_URL}?filter={filter_str}\"  # Filter data\n",
    "            f\"&select={','.join(select_data)}\"  # Select data\n",
    "            f\"&per_page={per_page}\"             # Fetch max results per request\n",
    "        )\n",
    "\n",
    "# author_count_filter = create_authors_filter([\"A5068556395\"])\n",
    "# query_filter = create_query_filter(concept_filter, cited_by_filter, author_count_filter) \n",
    "\n",
    "# test_works_url = get_works_url(query_filter, WORKS_ATTRIBUTES)\n",
    "# test_works_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_work(item) -> tuple[dict, dict]:\n",
    "\n",
    "    return {\n",
    "        \"id\": item[\"id\"],\n",
    "        \"publication_year\": item.get(\"publication_year\"),\n",
    "        \"cited_by_count\": item.get(\"cited_by_count\", 0),\n",
    "        \"author_ids\": [auth[\"author\"][\"id\"][id_slice:] for auth in item.get(\"authorships\", [])]\n",
    "    }, {\n",
    "        \"id\": item[\"id\"],\n",
    "        \"title\": item.get(\"title\"),\n",
    "        \"abstract_inverted_index\": item.get(\"abstract_inverted_index\")\n",
    "    }\n",
    "\n",
    "def fetch_work_batched(authors):\n",
    "    author_count_filter = create_authors_filter(authors)\n",
    "    query_filter = create_query_filter(concept_filter, cited_by_filter, author_count_filter) \n",
    "    url = get_works_url(query_filter, WORKS_ATTRIBUTES)\n",
    "\n",
    "    all_papers = []\n",
    "    all_abstracts = []\n",
    "\n",
    "    def process_work(work):\n",
    "        papers, abstracts = map_work(work)\n",
    "        all_papers.append(papers)\n",
    "        all_abstracts.append(abstracts)\n",
    "        return None\n",
    "\n",
    "    make_paginated_requests(url, mapper=process_work)\n",
    "\n",
    "    return all_papers, all_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching works for 918 authors in 37 batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a7033d16f9454b96eb41e3a0b604f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching works in parallel:   0%|          | 0/37 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching 37 results\n"
     ]
    }
   ],
   "source": [
    "author_ids = df[\"id\"].tolist()\n",
    "author_batches = [author_ids[i: i + BATCH_SIZE] for i in range(0, len(author_ids), BATCH_SIZE)]\n",
    "\n",
    "print(f\"Fetching works for {len(author_ids)} authors in {len(author_batches)} batches\")\n",
    "\n",
    "results = Parallel(n_jobs=NUM_CORES)(\n",
    "    delayed(fetch_work_batched)(batch) for batch in tqdm(author_batches, desc=\"Fetching works in parallel\", unit=\"batch\")\n",
    ")\n",
    "\n",
    "print(f\"Finished fetching {len(results)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 12239 papers and 12239 abstracts\n"
     ]
    }
   ],
   "source": [
    "all_papers = [paper for batch_papers, _ in results for paper in batch_papers]\n",
    "all_abstracts = [abstract for _, batch_abstracts in results for abstract in batch_abstracts]\n",
    "\n",
    "print(f\"Got {len(all_papers)} papers and {len(all_abstracts)} abstracts\")\n",
    "\n",
    "papers_df = pd.DataFrame(all_papers)\n",
    "abstracts_df = pd.DataFrame(all_abstracts)\n",
    "\n",
    "papers_df = papers_df.drop_duplicates(subset='id', keep='first')\n",
    "abstracts_df = abstracts_df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "papers_df.to_csv(\"ic2s2_papers.csv\", index=False)\n",
    "abstracts_df.to_csv(\"ic2s2_abstract.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "WORKS_URL = \"https://api.openalex.org/works\"\n",
    "BATCH_SIZE = 25\n",
    "MAX_REQUESTS_PER_SECOND = 10\n",
    "NUM_CORES = 10\n",
    "\n",
    "social_science_ids, quantitative_ids = get_concept_ids() \n",
    "\n",
    "\n",
    "\n",
    "def is_invalid_work(work):\n",
    "    \"\"\"Check if a work meets all filtering criteria\"\"\"\n",
    "    try:\n",
    "        if work.get('cited_by_count', 0) <= 10:\n",
    "            return \"To few citations\"\n",
    "            \n",
    "        if len(work.get('authorships', [])) >= 10:\n",
    "            return \"To many authorships\"\n",
    "        \n",
    "        concepts = work.get('concepts', [])\n",
    "        level0_concepts = [c['id'] for c in concepts if c.get('level') == 0]\n",
    "        \n",
    "        has_social = any(c in social_science_ids for c in level0_concepts)\n",
    "        has_quant = any(c in quantitative_ids for c in level0_concepts)\n",
    "        \n",
    "        if not has_social or not has_quant:\n",
    "            return \"No social or quantitative concept\"\n",
    "\n",
    "        return False\n",
    "        \n",
    "    except Exception as ex:\n",
    "        return f\"Error validating work: {str(ex)}\"\n",
    "\n",
    "def fetch_work(batch):\n",
    "    \"\"\"Fetch works for a batch of authors, handling pagination.\"\"\"\n",
    "    batch_papers = []\n",
    "    batch_abstracts = []\n",
    "    cursor = \"*\"  # Start with '*' to get the first page\n",
    "\n",
    "    while cursor:  # Continue fetching until there's no cursor (no more pages)\n",
    "        query_url = (\n",
    "            f\"{WORKS_URL}?filter={get_query_filter(batch)}\"\n",
    "            f\"&select={select_data}\"\n",
    "            f\"&per_page=200\"  # Fetch max results per request\n",
    "            f\"&cursor={cursor}\"  # Use cursor for pagination\n",
    "        )\n",
    "\n",
    "        while True:  # Retry fetching if rate limited\n",
    "            response = requests.get(query_url)\n",
    "            if response.status_code == 429:\n",
    "                print(f\"Got rate limited, waiting for 0.5 second\")\n",
    "                sleep(0.5)\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        if not response.ok:\n",
    "            print(f\"Error fetching batch: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        json_data = response.json()\n",
    "\n",
    "        # Store retrieved works\n",
    "        for work in json_data.get(\"results\", []):\n",
    "            if (error := is_invalid_work(work)):\n",
    "                print(\"Work was not valid: \", error)\n",
    "                continue\n",
    "            batch_papers.append({\n",
    "                \"id\": work[\"id\"],\n",
    "                \"publication_year\": work.get(\"publication_year\"),\n",
    "                \"cited_by_count\": work.get(\"cited_by_count\", 0),\n",
    "                \"author_ids\": [auth[\"author\"][\"id\"] for auth in work.get(\"authorships\", [])]\n",
    "            })\n",
    "\n",
    "            batch_abstracts.append({\n",
    "                \"id\": work[\"id\"],\n",
    "                \"title\": work.get(\"title\"),\n",
    "                \"abstract_inverted_index\": work.get(\"abstract_inverted_index\")\n",
    "            })\n",
    "\n",
    "        cursor = json_data.get(\"meta\", {}).get(\"next_cursor\")\n",
    "\n",
    "        sleep(1 / MAX_REQUESTS_PER_SECOND)\n",
    "\n",
    "    return batch_papers, batch_abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting works from authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching works in parallel: 100%|██████████| 37/37 [00:08<00:00,  4.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 10199 papers\n",
      "Got 10199 abstracts\n"
     ]
    }
   ],
   "source": [
    "# Process authors in parallel batches\n",
    "author_ids = df[\"id\"].tolist()\n",
    "author_batches = [author_ids[i: i + BATCH_SIZE] for i in range(0, len(author_ids), BATCH_SIZE)]\n",
    "\n",
    "results = Parallel(n_jobs=NUM_CORES)(\n",
    "    delayed(fetch_work)(batch) for batch in tqdm(author_batches, desc=\"Fetching works in parallel\", unit=\"batch\")\n",
    ")\n",
    "\n",
    "# Flatten results\n",
    "all_papers = [paper for batch_papers, _ in results for paper in batch_papers]\n",
    "all_abstracts = [abstract for _, batch_abstracts in results for abstract in batch_abstracts]\n",
    "\n",
    "# Convert to DataFrame\n",
    "papers_df = pd.DataFrame(all_papers)\n",
    "abstracts_df = pd.DataFrame(all_abstracts)\n",
    "\n",
    "# Drop\n",
    "papers_df = papers_df.drop_duplicates(subset='id', keep='first')\n",
    "abstracts_df = abstracts_df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "# Save to CSV\n",
    "papers_df.to_csv(\"ic2s2_papers.csv\", index=False)\n",
    "abstracts_df.to_csv(\"ic2s2_abstract.csv\", index=False)\n",
    "\n",
    "print(f\"Got {len(papers_df)} papers\")\n",
    "print(f\"Got {len(abstracts_df)} abstracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15404"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df = pd.read_csv(\"ic2s2_papers.csv\", converters={'author_ids': literal_eval})\n",
    "all_author_ids = papers_df.explode('author_ids')[\"author_ids\"].unique().tolist()\n",
    "len(set(all_author_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from new authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "\n",
    "social_science_ids, quantitative_ids = get_concept_ids() \n",
    "\n",
    "select_data = \"id,display_name,works_count,summary_stats,affiliations,works_api_url\"\n",
    "\n",
    "def is_invalid_author(author):\n",
    "    \"\"\"Check if a work meets all filtering criteria\"\"\"\n",
    "    try:\n",
    "        num_works = author.get('works_count', 0)\n",
    "        if 5 >= num_works >= 5000:\n",
    "            return f\"To many works {num_works}\"\n",
    "\n",
    "        return False\n",
    "        \n",
    "    except Exception as ex:\n",
    "        return f\"Error validating work: {str(ex)}\"\n",
    "\n",
    "def fetch_authors(batch):\n",
    "    \"\"\"Fetch works for a batch of authors, handling pagination.\"\"\"\n",
    "    batch_authors = []\n",
    "    cursor = \"*\"\n",
    "\n",
    "    while cursor:  # Continue fetching until there's no cursor (no more pages)\n",
    "        query_url = (\n",
    "            f\"{AUTHORS_URL}?filter=id:{'|'.join(batch)}\"\n",
    "            f\"&select={select_data}\"\n",
    "            f\"&per_page={100}\"  # Fetch max results per request\n",
    "            f\"&cursor={cursor}\"  # Use cursor for pagination\n",
    "        )\n",
    "\n",
    "        print(query_url)\n",
    "\n",
    "        while True:  # Retry fetching if rate limited\n",
    "            response = requests.get(query_url)\n",
    "            if response.status_code == 429:\n",
    "                print(f\"Got rate limited, waiting for 0.5 second\")\n",
    "                sleep(0.5)\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        if not response.ok:\n",
    "            print(f\"Error fetching batch: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        json_data = response.json()\n",
    "\n",
    "        # Store retrieved works\n",
    "        for author in json_data.get(\"results\", []):\n",
    "            if (error := is_invalid_author(author)):\n",
    "                print(\"author was not valid: \", error)\n",
    "                continue\n",
    "            batch_authors.append({\n",
    "                \"id\": author[\"id\"],\n",
    "                \"display_name\": author.get(\"display_name\"),\n",
    "                \"works_count\": author.get(\"works_count\"),\n",
    "            })\n",
    "\n",
    "        cursor = None # json_data.get(\"meta\", {}).get(\"next_cursor\")\n",
    "        if cursor:\n",
    "            print(f\"Fetching next page: {cursor}\")\n",
    "\n",
    "        sleep(1 / MAX_REQUESTS_PER_SECOND)\n",
    "\n",
    "    return batch_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing to fetch authors:15404\n",
      "Fetching 309 batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e039b5b9474a7ebdb1d1ac9b13285c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching works in parallel:   0%|          | 0/309 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m author_batches \u001b[38;5;241m=\u001b[39m [author_ids[i: i \u001b[38;5;241m+\u001b[39m AUTHOR_BATCH_SIZE] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(author_ids), AUTHOR_BATCH_SIZE)]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(author_batches)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m batches\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_CORES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfetch_authors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthor_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFetching works in parallel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone fetching authors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m all_author_data \u001b[38;5;241m=\u001b[39m [paper \u001b[38;5;28;01mfor\u001b[39;00m batch_papers \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mfor\u001b[39;00m paper \u001b[38;5;129;01min\u001b[39;00m batch_papers]\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "AUTHOR_BATCH_SIZE = 50\n",
    "\n",
    "author_ids = all_author_ids\n",
    "\n",
    "print(f\"preparing to fetch authors:{len(author_ids)}\")\n",
    "\n",
    "author_batches = [author_ids[i: i + AUTHOR_BATCH_SIZE] for i in range(0, len(author_ids), AUTHOR_BATCH_SIZE)]\n",
    "\n",
    "print(f\"Fetching {len(author_batches)} batches\")\n",
    "\n",
    "results = Parallel(n_jobs=NUM_CORES)(\n",
    "    delayed(fetch_authors)(batch) for batch in tqdm(author_batches, desc=\"Fetching works in parallel\", unit=\"batch\")\n",
    ")\n",
    "\n",
    "print(\"Done fetching authors\")\n",
    "\n",
    "all_author_data = [paper for batch_papers in results for paper in batch_papers]\n",
    "\n",
    "# Convert to DataFrame\n",
    "all_author_df = pd.DataFrame(all_author_data)\n",
    "\n",
    "# Drop\n",
    "all_author_df = all_author_df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "# Save to CSV\n",
    "all_author_df.to_csv(\"with_coauthors.csv\", index=False)\n",
    "\n",
    "print(f\"Got {len(all_author_df)} authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14175"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_author_df[\"works_count\"].describe()\n",
    "all_author_df = all_author_df[all_author_df[\"works_count\"] >= 5]\n",
    "all_author_df = all_author_df[all_author_df[\"works_count\"] <= 5000]\n",
    "all_author_df.to_csv(\"with_coauthors_filtered.csv\", index=False)\n",
    "all_filtered_author_ids = all_author_df[\"id\"].tolist()\n",
    "len(all_filtered_author_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Network of Computational Social Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting final dataset with authors and coauthors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15404"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# papers_df = pd.read_csv(\"ic2s2_papers.csv\", converters={'author_ids': literal_eval})\n",
    "# all_author_ids = papers_df.explode('author_ids')[\"author_ids\"].unique().tolist()\n",
    "# len(set(all_author_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 567 batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5fa2a55f6d5498383b476ce46838b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching works in parallel:   0%|          | 0/567 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "BATCH_SIZE = 25\n",
    "\n",
    "all_author_batches_ids = [all_filtered_author_ids[i: i + BATCH_SIZE] for i in range(0, len(all_filtered_author_ids), BATCH_SIZE)]\n",
    "\n",
    "print(f\"Fetching {len(all_author_batches_ids)} batches\")\n",
    "\n",
    "results = Parallel(n_jobs=NUM_CORES)(\n",
    "    delayed(fetch_work)(batch) for batch in tqdm(all_author_batches_ids, desc=\"Fetching works in parallel\", unit=\"batch\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers = [abstract for batch_papers, _ in results for abstract in batch_papers]\n",
    "all_abstracts = [abstract for _, batch_abstracts in results for abstract in batch_abstracts]\n",
    "\n",
    "# Convert to DataFrame\n",
    "all_papers_df = pd.DataFrame(all_papers)\n",
    "all_abstracts_df = pd.DataFrame(all_abstracts)\n",
    "\n",
    "# Drop\n",
    "all_papers_df = all_papers_df.drop_duplicates(subset='id', keep='first')\n",
    "all_abstracts_df = all_abstracts_df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "all_papers_df.to_csv(\"all_papers.csv\", index=False)\n",
    "all_abstracts_df.to_csv(\"all_abstracts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184291"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_papers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df.to_csv(\"ic2s2_coauthors_papers.csv\", index=False)\n",
    "# abstracts_df.to_csv(\"ic2s2_coauthors_abstracts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 184455 number of papers\n"
     ]
    }
   ],
   "source": [
    "papers_coauthor_df = pd.read_csv(\"ic2s2_coauthors_papers.csv\", converters={'author_ids': literal_eval})\n",
    "# abstracts_coauthor_df = pd.read_csv(\"ic2s2_coauthors_abstracts.csv\")\n",
    "\n",
    "print(f\"Got {len(papers_coauthor_df)} number of papers\")\n",
    "# print(f\"Got {len(abstracts_coauthor_df)} number of abstracts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Network Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting author pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = Counter()\n",
    "\n",
    "valid_authors = set(df[\"id\"])\n",
    "\n",
    "filtered_papers = papers_coauthor_df[\n",
    "    papers_coauthor_df[\"author_ids\"].apply(lambda authors: all(a in valid_authors for a in authors))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "for author_list in filtered_papers[\"author_ids\"]:\n",
    "    for pair in combinations(author_list, 2):\n",
    "        edges[pair] += 1\n",
    "\n",
    "edgelist = [(a, b, count) for (a, b), count in edges.items()]\n",
    "len(edgelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graph(graph_file, G):\n",
    "    data = nx.readwrite.json_graph.node_link_data(G)\n",
    "    with open(graph_file, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def load_graph(graph_file):\n",
    "    with open(graph_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return nx.readwrite.json_graph.node_link_graph(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "924\n",
      "924\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "df = df.drop_duplicates(subset='id', keep='first')\n",
    "print(len(df))\n",
    "\n",
    "df_exploded = papers_coauthor_df.explode(\"author_ids\")\n",
    "\n",
    "author_stats = df_exploded.groupby(\"author_ids\").agg(\n",
    "    first_publication_year=(\"publication_year\", \"min\"),\n",
    "    cited_by_count=(\"cited_by_count\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "df_merged = df.merge(author_stats, left_on=\"id\", right_on=\"author_ids\", how=\"inner\")\n",
    "df_merged.drop(columns=[\"author_ids\"], inplace=True)\n",
    "attr_dict = df_merged[[\"id\", \"display_name\", \"country_code\", \"first_publication_year\", \"cited_by_count\"]].set_index(\"id\").to_dict(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_file = \"ic2s2_coauthors_graph.json\"\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(edgelist)\n",
    "nx.set_node_attributes(G, attr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_graph(graph_file, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preliminary Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 123 links between 225626 nodes\n",
      "Network density is: 0.020664869721473494\n",
      "Is fully connected:  False\n",
      "Number of connected components:  28\n",
      "Number of isolated nodes:  0\n"
     ]
    }
   ],
   "source": [
    "# Network Stats\n",
    "num_links = len(edgelist)\n",
    "num_nodes = len(set(papers_coauthor_df.explode('author_ids')[\"author_ids\"].unique().tolist()))\n",
    "print(f\"Got {num_links} links between {num_nodes} nodes\")\n",
    "\n",
    "# Density Stats\n",
    "print(f'Network density is: {nx.density(G)}')\n",
    "\n",
    "# Number of connected components\n",
    "num_isolated = len(list(nx.isolates(G)))\n",
    "print(\"Is fully connected: \", nx.is_connected(G))\n",
    "print(\"Number of connected components: \", nx.number_connected_components(G))\n",
    "print(\"Number of isolated nodes: \", num_isolated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg': 2.169811320754717, 'median': 2.0, 'mode': 1, 'min': 1, 'max': 10}\n",
      "{'avg': 3.830188679245283, 'median': 2.0, 'mode': 2, 'min': 1, 'max': 48}\n"
     ]
    }
   ],
   "source": [
    "degrees = [d for _, d in G.degree()]\n",
    "strengths = [s for _, s in G.degree(weight=\"weight\")]\n",
    "\n",
    "degree_stats = {\n",
    "    \"avg\": np.mean(degrees),\n",
    "    \"median\": np.median(degrees),\n",
    "    \"mode\": Counter(degrees).most_common(1)[0][0],\n",
    "    \"min\": np.min(degrees),\n",
    "    \"max\": np.max(degrees)\n",
    "}\n",
    "\n",
    "strength_stats = {\n",
    "    \"avg\": np.mean(strengths),\n",
    "    \"median\": np.median(strengths),\n",
    "    \"mode\": Counter(strengths).most_common(1)[0][0],\n",
    "    \"min\": np.min(strengths),\n",
    "    \"max\": np.max(strengths)\n",
    "}\n",
    "\n",
    "print(degree_stats)\n",
    "print(strength_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://openalex.org/A5056499434', 10), ('https://openalex.org/A5000679279', 8), ('https://openalex.org/A5082698243', 7), ('https://openalex.org/A5026949484', 6), ('https://openalex.org/A5020533147', 6)]\n"
     ]
    }
   ],
   "source": [
    "def top_nodes_by_degree(G, top_n=5):\n",
    "    return sorted(G.degree, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "top_5 = top_nodes_by_degree(G)\n",
    "print(top_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"zoom\": 0.6,\n",
    "    \"scale_node_size_by_strength\": True,\n",
    "    \"node_size_variation\": 1,\n",
    "    \"node_size\": 30,\n",
    "    \"node_gravity\": 0.45,\n",
    "}\n",
    "\n",
    "id_to_name = pd.Series(df.display_name.values, index=df.id).to_dict()\n",
    "\n",
    "G_named = nx.relabel_nodes(G, id_to_name)\n",
    "\n",
    "network, config = nw.visualize(G_named, config=config)\n",
    "\n",
    "# fig, ax = nw.draw_netwulf(network, figsize=(10,10))\n",
    "plt.show()\n",
    "# plt.savefig(\"myfigure.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
