{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "> **Github repository**: [02467_Assignment1](https://github.com/JulWin24/02467_Assignment1)\n",
    ">\n",
    "> **Group members**:\n",
    "> - Rune Harlyk (s234814)\n",
    "> - Joseph Nguyen (s234826)\n",
    "> - Julius Winkel (s234862)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from joblib import Parallel, delayed \n",
    "from bs4 import BeautifulSoup\n",
    "from unidecode import unidecode\n",
    "from fuzzywuzzy import fuzz\n",
    "from collections import defaultdict\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval \n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import netwulf as nw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ic2s2-2023.org/program\"\n",
    "\n",
    "req = requests.get(url)\n",
    "soup = BeautifulSoup(req.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: 1475 plenary names\n",
      "Found: 10 keynotes names\n",
      "Found: 49 chair names\n",
      "Found: 1491 names in total\n"
     ]
    }
   ],
   "source": [
    "names = set()\n",
    "\n",
    "def get_plenary_names(names, soup): \n",
    "    new_names = {name.strip() for nav_list in soup.find_all(\"ul\", class_=\"nav_list\") \n",
    "        for i in nav_list.find_all(\"i\") \n",
    "        for name in i.get_text(strip=True).split(\",\")}\n",
    "    print(f\"Found: {len(new_names)} plenary names\")\n",
    "    names.update(new_names)\n",
    "\n",
    "def get_keynotes_names(names, soup):\n",
    "    new_names = {a.get_text(strip=True).replace(\"Keynote - \", \"\") \n",
    "        for a in soup.find_all(\"a\", href=lambda x: x and x.startswith(\"/keynotes#\"))}\n",
    "    print(f\"Found: {len(new_names)} keynotes names\")\n",
    "    names.update(new_names)\n",
    "    \n",
    "def get_chair_names(names, soup):\n",
    "    new_names = {i.get_text(strip=True).replace(\"Chair: \", \"\") \n",
    "          for i in soup.find_all(\"i\") if i.get_text(strip=True).startswith(\"Chair:\")}\n",
    "    print(f\"Found: {len(new_names)} chair names\")\n",
    "    names.update(new_names)\n",
    "\n",
    "get_plenary_names(names, soup)\n",
    "get_keynotes_names(names, soup)\n",
    "get_chair_names(names, soup)\n",
    "\n",
    "print(f\"Found: {len(names)} names in total\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: 1486 names\n",
      "After fuzzing: 1460 names\n"
     ]
    }
   ],
   "source": [
    "def clean_name(name):\n",
    "    name = unidecode(name)\n",
    "    return name\n",
    "\n",
    "def clean_names(names):\n",
    "    names = {clean_name(name) for name in names}\n",
    "    return names\n",
    "\n",
    "def fuzz_names(names, threshold=90):\n",
    "    names_list = sorted(names)\n",
    "    name_groups = defaultdict(list)\n",
    "\n",
    "    for name in names_list:\n",
    "        first_letter = name[0] if name else \"\"\n",
    "        name_groups[first_letter].append(name)\n",
    "\n",
    "    merge_map = {}\n",
    "    for letter, group in name_groups.items():\n",
    "        for i, name in enumerate(group):\n",
    "            for j in range(i + 1, len(group)):\n",
    "                match_name = group[j]\n",
    "                score = fuzz.ratio(name, match_name)\n",
    "                if score >= threshold:\n",
    "                    merge_map[match_name] = name\n",
    "\n",
    "    merged_names = set()\n",
    "    for name in names_list:\n",
    "        standardized_name = merge_map.get(name, name)\n",
    "        merged_names.add(standardized_name)\n",
    "\n",
    "    return merged_names\n",
    "\n",
    "names = clean_names(names)\n",
    "print(f\"After cleaning: {len(names)} names\")\n",
    "\n",
    "names = fuzz_names(names)\n",
    "print(f\"After fuzzing: {len(names)} names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('author_names_2023.txt', 'w', encoding=\"utf8\") as f:\n",
    "    for name in sorted(names):\n",
    "        f.write(f\"{name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Ready Made vs Custom Made Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading researches 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded names: 1206\n",
      "After fuzzing: 1202 names\n"
     ]
    }
   ],
   "source": [
    "names_file = \"author_names_2024.txt\"\n",
    "data_file = \"author_data.csv\"\n",
    "\n",
    "with open(names_file, 'r', encoding=\"utf8\") as f:\n",
    "    names = f.read().splitlines()\n",
    "\n",
    "print(f\"Loaded names: {len(names)}\")\n",
    "names = clean_names(names)\n",
    "\n",
    "names = fuzz_names(names)\n",
    "print(f\"After fuzzing: {len(names)} names\")\n",
    "\n",
    "# TODO\n",
    "# 1 - Remove (Santa Fe Institute) from names\n",
    "# 2 - Remove Pensylvania State University from names\n",
    "\n",
    "names = sorted(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching author data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json(nested_data, parent_key='', sep='_', keep_path=False):\n",
    "    flat_dict = {}\n",
    "    for k, v in nested_data.items():\n",
    "        new_key = k if not keep_path else f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            flat_dict.update(flatten_json(v, new_key, sep))\n",
    "        elif isinstance(v, list):\n",
    "            for i, item in enumerate(v):\n",
    "                if isinstance(item, dict):\n",
    "                    flat_dict.update(flatten_json(item, f\"{new_key}{sep}{i}\", sep))\n",
    "        else:\n",
    "            flat_dict[new_key] = v\n",
    "    return flat_dict\n",
    "\n",
    "def filter_json(data, template):\n",
    "    if isinstance(template, dict):\n",
    "        return {k: filter_json(data.get(k, None), v) for k, v in template.items() if k in data}\n",
    "    elif isinstance(template, list) and isinstance(data, list):\n",
    "        return [filter_json(item, template[0]) for item in data if isinstance(item, dict)]\n",
    "    return data\n",
    "\n",
    "def extract_allowed(data, allowed_keys):\n",
    "    result = {}\n",
    "    for k, v in data.items():\n",
    "        if k in allowed_keys:\n",
    "            result[k] = v\n",
    "        elif isinstance(v, dict):\n",
    "            result.update(extract_allowed(v, allowed_keys))\n",
    "    return result\n",
    "\n",
    "def load_existing_data():\n",
    "    if os.path.exists(data_file):\n",
    "        return pd.read_csv(data_file).to_dict(orient=\"records\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already have 132, missing 1091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1091/1091 [04:08<00:00,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got data for: 1115, missing 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "session = requests.Session()\n",
    "\n",
    "template = {\n",
    "    \"id\": None,\n",
    "    \"display_name\": None,\n",
    "    \"works_count\": None,\n",
    "    \"summary_stats\": {\"h_index\": None},\n",
    "    \"affiliations\": [{\"institution\": {\"country_code\": None}}],\n",
    "    \"works_api_url\": None,\n",
    "}\n",
    "\n",
    "def get_author_data(name):\n",
    "    try:\n",
    "        response = session.get(f\"https://api.openalex.org/authors?filter=display_name.search:{name}\", timeout=5)\n",
    "        sleep(0.05)\n",
    "        if not response.ok:\n",
    "            print(response.status_code)\n",
    "            return name\n",
    "        json_data = response.json()\n",
    "        if not json_data['results']:\n",
    "            return name\n",
    "        return flatten_json(filter_json(json_data['results'][0], template))\n",
    "    except Exception as ex:\n",
    "        return name\n",
    "\n",
    "existing_data = load_existing_data()\n",
    "existing_names = {entry['display_name'] for entry in existing_data if 'display_name' in entry}\n",
    "names_to_process = list(set(names) - existing_names)\n",
    "print(f\"Already have {len(existing_names)}, missing {len(names_to_process)}\")\n",
    "author_data = existing_data\n",
    "bad_names = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    futures = {executor.submit(get_author_data, name): name for name in names_to_process}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        result = future.result()\n",
    "        if isinstance(result, dict):\n",
    "            author_data.append(result)\n",
    "        else:\n",
    "            bad_names.append(result)\n",
    "\n",
    "pd.DataFrame(author_data).to_csv(data_file, index=False)\n",
    "print(f\"Got data for: {len(author_data)}, missing {len(bad_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data again and filter between 5-5000 works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115\n",
      "941\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('author_data.csv')\n",
    "\n",
    "print(len(df))\n",
    "df = df[(df[\"works_count\"] >= 5) & (df[\"works_count\"] <= 5000)]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_science_fields = ['Political science', 'Economics', 'Psychology', 'Sociology']\n",
    "quantitative_fields = ['Mathematics', 'Physics', 'Computer science']\n",
    "min_cited_by = 10\n",
    "max_authors = 10\n",
    "\n",
    "\n",
    "def get_concept_ids():\n",
    "    concepts_url = \"https://api.openalex.org/concepts?filter=level:0&per-page=200\"\n",
    "    response_concepts = requests.get(concepts_url)\n",
    "\n",
    "\n",
    "    if response_concepts.ok:\n",
    "        concepts = response_concepts.json()['results']\n",
    "        \n",
    "        social_science_ids = [i['id'] for i in concepts if i['display_name'] in social_science_fields]\n",
    "        quantitative_ids = [i['id'] for i in concepts if i['display_name'] in quantitative_fields]\n",
    "\n",
    "    return social_science_ids, quantitative_ids\n",
    "\n",
    "def create_concept_filter():\n",
    "    social_science_ids, quantitative_ids = get_concept_ids() \n",
    "    social_science_filter = '|'.join(social_science_ids)\n",
    "    quantitative_filter = '|'.join(quantitative_ids)\n",
    "\n",
    "    concept_filter = f\"concepts.id:{social_science_filter},concepts.id:{quantitative_filter}\"\n",
    "    return concept_filter\n",
    "\n",
    "def create_authors_filter(ids):\n",
    "    return f\"authorships.author.id:{'|'.join(ids)}\"\n",
    "\n",
    "concept_filter = create_concept_filter()\n",
    "cited_by_filter = f\"cited_by_count:>{min_cited_by}\"\n",
    "author_count_filter = f\"authors_count:<{max_authors}\"\n",
    "\n",
    "def get_query_filter(ids):\n",
    "    return \",\".join((concept_filter, cited_by_filter, author_count_filter, create_authors_filter(ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "WORKS_URL = \"https://api.openalex.org/works\"\n",
    "BATCH_SIZE = 25\n",
    "MAX_REQUESTS_PER_SECOND = 10\n",
    "NUM_CORES = 10\n",
    "\n",
    "select_data = \"id,title,publication_year,abstract_inverted_index,authorships,cited_by_count,concepts\"\n",
    "\n",
    "def fetch_works(batch):\n",
    "    \"\"\"Fetch works for a batch of authors, handling pagination.\"\"\"\n",
    "    batch_papers = []\n",
    "    batch_abstracts = []\n",
    "    cursor = \"*\"  # Start with '*' to get the first page\n",
    "\n",
    "    while cursor:  # Continue fetching until there's no cursor (no more pages)\n",
    "        query_url = (\n",
    "            f\"{WORKS_URL}?filter={get_query_filter(batch)}\"\n",
    "            f\"&select={select_data}\"\n",
    "            f\"&per_page=200\"  # Fetch max results per request\n",
    "            f\"&cursor={cursor}\"  # Use cursor for pagination\n",
    "        )\n",
    "\n",
    "        while True:  # Retry fetching if rate limited\n",
    "            response = requests.get(query_url)\n",
    "            if response.status_code == 429:\n",
    "                print(f\"Got rate limited, waiting for 0.5 second\")\n",
    "                sleep(0.5)\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        if not response.ok:\n",
    "            print(f\"Error fetching batch: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        json_data = response.json()\n",
    "\n",
    "        # Store retrieved works\n",
    "        for work in json_data.get(\"results\", []):\n",
    "            batch_papers.append({\n",
    "                \"id\": work[\"id\"],\n",
    "                \"publication_year\": work.get(\"publication_year\"),\n",
    "                \"cited_by_count\": work.get(\"cited_by_count\", 0),\n",
    "                \"author_ids\": [auth[\"author\"][\"id\"] for auth in work.get(\"authorships\", [])]\n",
    "            })\n",
    "\n",
    "            batch_abstracts.append({\n",
    "                \"id\": work[\"id\"],\n",
    "                \"title\": work.get(\"title\"),\n",
    "                \"abstract_inverted_index\": work.get(\"abstract_inverted_index\")\n",
    "            })\n",
    "\n",
    "        cursor = json_data.get(\"meta\", {}).get(\"next_cursor\")\n",
    "\n",
    "        sleep(1 / MAX_REQUESTS_PER_SECOND)\n",
    "\n",
    "    return batch_papers, batch_abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting works from authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Fetching works in parallel: 100%|██████████| 38/38 [00:10<00:00,  3.68batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 10173 papers\n",
      "Got 10173 abstracts\n"
     ]
    }
   ],
   "source": [
    "# Process authors in parallel batches\n",
    "author_ids = df[\"id\"].tolist()\n",
    "author_batches = [author_ids[i: i + BATCH_SIZE] for i in range(0, len(author_ids), BATCH_SIZE)]\n",
    "\n",
    "results = Parallel(n_jobs=NUM_CORES)(\n",
    "    delayed(fetch_works)(batch) for batch in tqdm(author_batches, desc=\"Fetching works in parallel\", unit=\"batch\")\n",
    ")\n",
    "\n",
    "# Flatten results\n",
    "all_papers = [paper for batch_papers, _ in results for paper in batch_papers]\n",
    "all_abstracts = [abstract for _, batch_abstracts in results for abstract in batch_abstracts]\n",
    "\n",
    "# Convert to DataFrame\n",
    "papers_df = pd.DataFrame(all_papers)\n",
    "abstracts_df = pd.DataFrame(all_abstracts)\n",
    "\n",
    "# Drop\n",
    "papers_df = papers_df.drop_duplicates(subset='id', keep='first')\n",
    "abstracts_df = abstracts_df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "# Save to CSV\n",
    "papers_df.to_csv(\"ic2s2_papers.csv\", index=False)\n",
    "abstracts_df.to_csv(\"ic2s2_abstract.csv\", index=False)\n",
    "\n",
    "print(f\"Got {len(papers_df)} papers\")\n",
    "print(f\"Got {len(abstracts_df)} abstracts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Network of Computational Social Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting final dataset with authors and coauthors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15361"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df = pd.read_csv(\"ic2s2_papers.csv\", converters={'author_ids': literal_eval})\n",
    "all_author_ids = papers_df.explode('author_ids')[\"author_ids\"].unique().tolist()\n",
    "len(set(all_author_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching works in parallel: 100%|██████████| 615/615 [04:39<00:00,  2.20batch/s]\n"
     ]
    }
   ],
   "source": [
    "all_author_batches_ids = [all_author_ids[i: i + BATCH_SIZE] for i in range(0, len(all_author_ids), BATCH_SIZE)]\n",
    "\n",
    "results = Parallel(n_jobs=NUM_CORES)(\n",
    "    delayed(fetch_works)(batch) for batch in tqdm(all_author_batches_ids, desc=\"Fetching works in parallel\", unit=\"batch\")\n",
    ")\n",
    "\n",
    "all_papers = [paper for batch_papers, _ in results for paper in batch_papers]\n",
    "# all_abstracts = [abstract for _, batch_abstracts in results for abstract in batch_abstracts]\n",
    "\n",
    "# Convert to DataFrame\n",
    "papers_df = pd.DataFrame(all_papers)\n",
    "# abstracts_df = pd.DataFrame(all_abstracts)\n",
    "\n",
    "# Drop\n",
    "papers_df = papers_df.drop_duplicates(subset='id', keep='first')\n",
    "# abstracts_df = abstracts_df.drop_duplicates(subset='id', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df.to_csv(\"ic2s2_coauthors_papers.csv\", index=False)\n",
    "# abstracts_df.to_csv(\"ic2s2_coauthors_abstracts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 184455 number of papers\n"
     ]
    }
   ],
   "source": [
    "papers_coauthor_df = pd.read_csv(\"ic2s2_coauthors_papers.csv\", converters={'author_ids': literal_eval})\n",
    "# abstracts_coauthor_df = pd.read_csv(\"ic2s2_coauthors_abstracts.csv\")\n",
    "\n",
    "print(f\"Got {len(papers_coauthor_df)} number of papers\")\n",
    "# print(f\"Got {len(abstracts_coauthor_df)} number of abstracts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Network Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting author pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = Counter()\n",
    "\n",
    "valid_authors = set(df[\"id\"])\n",
    "\n",
    "filtered_papers = papers_coauthor_df[\n",
    "    papers_coauthor_df[\"author_ids\"].apply(lambda authors: all(a in valid_authors for a in authors))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "for author_list in filtered_papers[\"author_ids\"]:\n",
    "    for pair in combinations(author_list, 2):\n",
    "        edges[pair] += 1\n",
    "\n",
    "edgelist = [(a, b, count) for (a, b), count in edges.items()]\n",
    "len(edgelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graph(graph_file, G):\n",
    "    data = nx.readwrite.json_graph.node_link_data(G)\n",
    "    with open(graph_file, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def load_graph(graph_file):\n",
    "    with open(graph_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return nx.readwrite.json_graph.node_link_graph(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "924\n",
      "924\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "df = df.drop_duplicates(subset='id', keep='first')\n",
    "print(len(df))\n",
    "\n",
    "df_exploded = papers_coauthor_df.explode(\"author_ids\")\n",
    "\n",
    "author_stats = df_exploded.groupby(\"author_ids\").agg(\n",
    "    first_publication_year=(\"publication_year\", \"min\"),\n",
    "    cited_by_count=(\"cited_by_count\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "df_merged = df.merge(author_stats, left_on=\"id\", right_on=\"author_ids\", how=\"inner\")\n",
    "df_merged.drop(columns=[\"author_ids\"], inplace=True)\n",
    "attr_dict = df_merged[[\"id\", \"display_name\", \"country_code\", \"first_publication_year\", \"cited_by_count\"]].set_index(\"id\").to_dict(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_file = \"ic2s2_coauthors_graph.json\"\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(edgelist)\n",
    "nx.set_node_attributes(G, attr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_graph(graph_file, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preliminary Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 123 links between 225626 nodes\n",
      "Network density is: 0.020664869721473494\n",
      "Is fully connected:  False\n",
      "Number of connected components:  28\n",
      "Number of isolated nodes:  0\n"
     ]
    }
   ],
   "source": [
    "# Network Stats\n",
    "num_links = len(edgelist)\n",
    "num_nodes = len(set(papers_coauthor_df.explode('author_ids')[\"author_ids\"].unique().tolist()))\n",
    "print(f\"Got {num_links} links between {num_nodes} nodes\")\n",
    "\n",
    "# Density Stats\n",
    "print(f'Network density is: {nx.density(G)}')\n",
    "\n",
    "# Number of connected components\n",
    "num_isolated = len(list(nx.isolates(G)))\n",
    "print(\"Is fully connected: \", nx.is_connected(G))\n",
    "print(\"Number of connected components: \", nx.number_connected_components(G))\n",
    "print(\"Number of isolated nodes: \", num_isolated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg': 2.169811320754717, 'median': 2.0, 'mode': 1, 'min': 1, 'max': 10}\n",
      "{'avg': 3.830188679245283, 'median': 2.0, 'mode': 2, 'min': 1, 'max': 48}\n"
     ]
    }
   ],
   "source": [
    "degrees = [d for _, d in G.degree()]\n",
    "strengths = [s for _, s in G.degree(weight=\"weight\")]\n",
    "\n",
    "degree_stats = {\n",
    "    \"avg\": np.mean(degrees),\n",
    "    \"median\": np.median(degrees),\n",
    "    \"mode\": Counter(degrees).most_common(1)[0][0],\n",
    "    \"min\": np.min(degrees),\n",
    "    \"max\": np.max(degrees)\n",
    "}\n",
    "\n",
    "strength_stats = {\n",
    "    \"avg\": np.mean(strengths),\n",
    "    \"median\": np.median(strengths),\n",
    "    \"mode\": Counter(strengths).most_common(1)[0][0],\n",
    "    \"min\": np.min(strengths),\n",
    "    \"max\": np.max(strengths)\n",
    "}\n",
    "\n",
    "print(degree_stats)\n",
    "print(strength_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://openalex.org/A5056499434', 10), ('https://openalex.org/A5000679279', 8), ('https://openalex.org/A5082698243', 7), ('https://openalex.org/A5026949484', 6), ('https://openalex.org/A5020533147', 6)]\n"
     ]
    }
   ],
   "source": [
    "def top_nodes_by_degree(G, top_n=5):\n",
    "    return sorted(G.degree, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "top_5 = top_nodes_by_degree(G)\n",
    "print(top_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"zoom\": 0.6,\n",
    "    \"scale_node_size_by_strength\": True,\n",
    "    \"node_size_variation\": 1,\n",
    "    \"node_size\": 30,\n",
    "    \"node_gravity\": 0.45,\n",
    "}\n",
    "\n",
    "id_to_name = pd.Series(df.display_name.values, index=df.id).to_dict()\n",
    "\n",
    "G_named = nx.relabel_nodes(G, id_to_name)\n",
    "\n",
    "network, config = nw.visualize(G_named, config=config)\n",
    "\n",
    "# fig, ax = nw.draw_netwulf(network, figsize=(10,10))\n",
    "plt.show()\n",
    "# plt.savefig(\"myfigure.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
